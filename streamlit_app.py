# streamlit_app.py
# (ì„ íƒ) gRPC ê²½ê³  ìˆ¨ê¸°ê¸°
import os
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GRPC_TRACE"] = ""

import sys
import asyncio
from pathlib import Path
from typing import Optional

import streamlit as st
from PIL import Image

from mcp.client.stdio import stdio_client
from mcp import ClientSession, StdioServerParameters
try:
    from langchain_mcp_adapters.tools import load_mcp_tools
except ImportError:
    from langchain_mcp.tools import load_mcp_tools  # í˜¸í™˜
from langgraph.prebuilt import create_react_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¸°ë³¸ UI/ìƒíƒœ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì†Œ", layout="wide")

ASSETS = Path(__file__).parent / "assets"
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY", "")

system_prompt = (
    "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë§ˆì¼€íŒ… ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê°€ë§¹ì ëª…ì„ ë°›ì•„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë°©ë¬¸ ê³ ê° í˜„í™©ì„ ë¶„ì„í•˜ê³ , "
    "ì ì ˆí•œ ë§ˆì¼€íŒ… ë°©ë²•/ì±„ë„/ë©”ì‹œì§€ë¥¼ ê°„ê²°í•˜ê²Œ ì¶”ì²œí•©ë‹ˆë‹¤. í‘œë¥¼ í™œìš©í•´ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”."
)
greeting = (
    "ë§ˆì¼€íŒ…ì´ í•„ìš”í•œ ê°€ë§¹ì ì„ ì•Œë ¤ì£¼ì„¸ìš”  \n"
    "(ì¡°íšŒê°€ëŠ¥ ì˜ˆì‹œ: ë™ëŒ€*, ìœ ìœ *, ë˜¥íŒŒ*, ë³¸ì£½*, ë³¸*, ì›ì¡°*, í¬ë§*, í˜ì´*, Hì»¤*, ì¼€í‚¤*)"
)

@st.cache_data(show_spinner=False)
def load_image(name: str) -> Optional[Image.Image]:
    p = ASSETS / name
    return Image.open(p) if p.exists() else None

def clear_chat_history():
    st.session_state.messages = [SystemMessage(content=system_prompt), AIMessage(content=greeting)]

with st.sidebar:
    img = load_image("shc_ci_basic_00.png")
    if img:
        st.image(img, use_container_width=True)
    st.markdown("<p style='text-align:center;'>2025 Big Contest</p>", unsafe_allow_html=True)
    st.markdown("<p style='text-align:center;'>AI DATA í™œìš©ë¶„ì•¼</p>", unsafe_allow_html=True)
    st.button("Clear Chat History", on_click=clear_chat_history)

st.title("ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ğŸ”‘ ë¹„ë°€ìƒë‹´ì†Œ")
st.subheader("#ìš°ë¦¬ë™ë„¤ #ìˆ¨ì€ë§›ì§‘ #ì†Œìƒê³µì¸ #ë§ˆì¼€íŒ… #ì „ëµ .. ğŸ¤¤")
hero = load_image("image_gen3.png")
if hero:
    st.image(hero, use_container_width=True, caption="ğŸŒ€ ë¨¸ë¦¬ì•„í”ˆ ë§ˆì¼€íŒ… ğŸ“Š ì–´ë–»ê²Œ í•˜ë©´ ì¢‹ì„ê¹Œ?")
st.write("")

if "messages" not in st.session_state:
    st.session_state.messages = [SystemMessage(content=system_prompt), AIMessage(content=greeting)]

for m in st.session_state.messages:
    role = "user" if isinstance(m, HumanMessage) else ("assistant" if isinstance(m, AIMessage) else "system")
    with st.chat_message(role):
        st.write(m.content)

def render_chat_message(role: str, content: str):
    with st.chat_message(role):
        st.markdown(str(content).replace("<br>", "  \n"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM & MCP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=GOOGLE_API_KEY,
    temperature=0.1,
)

# í˜„ì¬ venvì˜ python.exeë¡œ MCP ì„œë²„ ì‹¤í–‰ (Windows/PyCharm ì•ˆì „)
server_params = StdioServerParameters(
    command=sys.executable,
    args=[str((Path(__file__).parent / "mcp_server.py").resolve())],
    env=None,
)

async def process_user_input():
    """ì‚¬ìš©ì ì§ˆì˜ë¥¼ MCP+LLMë¡œ ì²˜ë¦¬"""
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            tools = await load_mcp_tools(session)
            agent = create_react_agent(llm, tools)
            result = await agent.ainvoke({"messages": st.session_state.messages})
            # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€
            msgs = [m for m in result["messages"] if isinstance(m, AIMessage)]
            return msgs[-1].content if msgs else "(ì‘ë‹µ ì—†ìŒ)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì…ë ¥ì°½
query = st.chat_input("ê°€ë§¹ì  ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”")
if query:
    st.session_state.messages.append(HumanMessage(content=query))
    render_chat_message("user", query)

    with st.spinner("Thinking..."):
        try:
            reply = asyncio.run(process_user_input())
            st.session_state.messages.append(AIMessage(content=reply))
            render_chat_message("assistant", reply)
        except* Exception as eg:
            # ì—¬ëŸ¬ ì˜ˆì™¸ ë¬¶ìŒ ì²˜ë¦¬ (Python 3.11+)
            for i, exc in enumerate(eg.exceptions, 1):
                msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ #{i}: {exc!r}"
                st.session_state.messages.append(AIMessage(content=msg))
                render_chat_message("assistant", msg)
